{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield Network Model of Associative Memory\n",
    "\n",
    "- See \n",
    "[Neuronal Dynamics by Gerstner - Chapter 17 Section 3](https://neuronaldynamics.epfl.ch/online/Ch17.S2.html) for an introduction to Hopfield networks.\n",
    "\n",
    "\n",
    "- The exercise is adapted from [here](https://neuronaldynamics-exercises.readthedocs.io/en/latest/exercises/hopfield-network.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import matplotlib.cbook\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "from neurodynex.hopfield_network import network, pattern_tools, plot_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Introduction to Hopfield Networks\n",
    "\n",
    "Here, neurons are pixels and take the values of -1 (*off*) or +1 (*on*). The network can store a certain number of pixel patterns, which is to be investigated in this exercise. During a retrieval phase, the network is started with some initial configuration and the network dynamics evolves towards the stored pattern (attractor) which is closest to the initial configuration.\n",
    "\n",
    "The dynamics is that of equation:\n",
    "\n",
    "$$S_i(t+1)=sgn\\left(\\sum_jw_{ij}S_j(t)\\right)$$\n",
    "\n",
    "In the Hopfield model each neuron is connected to every other neuron (full connectivity). The connection matrix is\n",
    "\n",
    "$$w_{ij} = \\frac{1}{N}\\sum_{\\mu} p_i^\\mu p_j^\\mu$$\n",
    "\n",
    "where N is the number of neurons, $p_i^\\mu$ is the value of neuron $i$ in pattern number $\\mu$ and the sum runs over all patterns from $\\mu = 1$ to $\\mu=P$. This is a simple correlation based learning rule (Hebbian learning). Since it is not a iterative rule it is sometimes called one-shot learning. The learning rule works best if the patterns that are to be stored are random patterns with equal probability for on (+1) and off (-1). In a large networks (N to infinity) the number of random patterns that can be stored is approximately 0.14 times N.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopfield networks can be analyzed mathematically. In this Python exercise we focus on visualization and simulation to develop our intuition about Hopfield dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We provide a couple of functions to easily create patterns, store them in the network and visualize the network dynamics. Check the modules `hopfield_network.network`, `hopfield_network.pattern_tools` and `hopfield_network.plot_tools` to learn the building blocks we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the following code. Read the inline comments and check the documentation. The patterns and the flipped pixels are randomly chosen. Therefore the result changes every time you execute this code. Run it several times and change some parameters like nr_patterns and nr_of_flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pattern_size = 5\n",
    "\n",
    "# create an instance of the class HopfieldNetwork\n",
    "hopfield_net = network.HopfieldNetwork(nr_neurons=pattern_size**2)\n",
    "# instantiate a pattern factory\n",
    "factory = pattern_tools.PatternFactory(pattern_size, pattern_size)\n",
    "# create a checkerboard pattern and add it to the pattern list\n",
    "checkerboard = factory.create_checkerboard()\n",
    "pattern_list = [checkerboard]\n",
    "# add random patterns to the list\n",
    "pattern_list.extend(factory.create_random_pattern_list(nr_patterns=3, on_probability=0.5))\n",
    "plot_tools.plot_pattern_list(pattern_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how similar are the random patterns and the checkerboard? Check the overlaps\n",
    "overlap_matrix = pattern_tools.compute_overlap_matrix(pattern_list)  # pairwise scalar product of flattened vectors\n",
    "plot_tools.plot_overlap_matrix(overlap_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let the hopfield network \"learn\" the patterns. Note: they are not stored\n",
    "# explicitly but only network weights are updated !\n",
    "hopfield_net.store_patterns(pattern_list)  # apply equation w_ij\n",
    "plot_tools.plot_nework_weights(hopfield_net) # visualizes the connectivity matrix hopfield_net.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a noisy version of a pattern and use that to initialize the network\n",
    "noisy_init_state = pattern_tools.flip_n(checkerboard, nr_of_flips=4)\n",
    "hopfield_net.set_state_from_pattern(noisy_init_state)\n",
    "plot_tools.plot_pattern(hopfield_net.state.reshape(5, 5)) # initial state of binary neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this initial state, let the network dynamics evolve.\n",
    "states = hopfield_net.run_with_monitoring(nr_steps=4)  # apply equation S_i\n",
    "# each network state is a vector. reshape it to the same shape used to create the patterns.\n",
    "states_as_patterns = factory.reshape_patterns(states)\n",
    "# plot the states of the network\n",
    "plot_tools.plot_state_sequence_and_overlap(states_as_patterns, pattern_list, reference_idx=0, suptitle=\"Network dynamics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - $N=4\\times 4$ Hopfield-network\n",
    "\n",
    "We study how a network stores and retrieve patterns. Using a small network of only 16 neurons allows us to have a close look at the network weights and dynamics.\n",
    "\n",
    "**Storing a single pattern.**\n",
    "\n",
    "Modify the Python code given above to implement this exercise:\n",
    "\n",
    "1.  Create a network with N=16 neurons.\n",
    "2.  Create a single 4 by 4 checkerboard pattern.\n",
    "3.  Store the checkerboard in the network.\n",
    "4.  Set the initial state of the network to a noisy version of the checkerboard (nr\\_flipped\\_pixels = 5).\n",
    "5.  Let the network dynamics evolve for 4 iterations.\n",
    "6.  Plot the sequence of network states along with the overlap of network state with the checkerboard.\n",
    "7. Now test whether the network can still retrieve the pattern if we increase the number of flipped pixels. What happens at `nr_flipped_pixels = 8`, what if `nr_flipped_pixels > 8`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The weights matrix.**\n",
    "\n",
    "The patterns a Hopfield network learns are not stored explicitly. Instead, the network learns by adjusting the weights to the pattern set it is presented during learning. Let's visualize this.\n",
    "\n",
    "8.  Create a new 4x4 network. Do not yet store any pattern.\n",
    "9.  What is the size of the network matrix?\n",
    "10.  Visualize the weight matrix using the function .plot\\_tools.plot\\_nework\\_weights. It takes the network as a parameter.\n",
    "11.  Create a checkerboard, store it in the network.\n",
    "12.  Plot the weights matrix. What weight values do occur?\n",
    "13.  Create a new 4x4 network\n",
    "14.  Create an L-shaped pattern (look at the pattern factory doc), store it in the network\n",
    "15.  Plot the weights matrix. What weight values do occur?\n",
    "16.  Create a new 4x4 network\n",
    "17. Create a checkerboard and an L-shaped pattern. Store **both** patterns in the network\n",
    "18. Plot the weights matrix. What weight values do occur? How does this matrix compare to the two previous matrices?\n",
    "\n",
    "The mapping of the 2 dimensional patterns onto the one dimensional list of network neurons is internal to the implementation of the network. You cannot know which pixel (x,y) in the pattern corresponds to which network neuron i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights Distribution.**\n",
    "\n",
    "19. It's interesting to look at the weights distribution in the three previous cases. You can easily plot a histogram by adding the following two lines to your script. It assumes you have stored your network in the variable 'hopfield\\_net'.\n",
    "\n",
    "``` sourceCode\n",
    "plt.figure()\n",
    "plt.hist(hopfield_net.weights.flatten())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Capacity of an N=100 Hopfield-network\n",
    "\n",
    "Larger networks can store more patterns. There is a theoretical limit: the capacity of the Hopfield network. Read [chapter \"17.2.4 Memory capacity\"](http://neuronaldynamics.epfl.ch/online/Ch17.S2.html) to learn how memory retrieval, pattern completion and the network capacity are related.\n",
    "\n",
    "1. A Hopfield network implements so called **associative** or **content-adressable** memory. Explain what this means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the value $C_\\text{store}$ given in the book, how many patterns can you store in a N=10x10 network? Use this number **K** in the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create an N=10x10 network and store a checkerboard pattern together with **(K-1) random patterns**. Then initialize the network with the **unchanged** checkerboard pattern. Let the network evolve for five iterations. Rerun your script a few times. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Non-random patterns\n",
    "\n",
    "In the previous exercises we used random patterns. Now we use a list of structured patterns: the letters A to Z. Each letter is represented in a 10 by 10 grid.\n",
    "\n",
    "<img src=https://neuronaldynamics-exercises.readthedocs.io/en/latest/_images/HF_LetterAandOverlap.png width=\"500\">\n",
    "<i><center>Eight letters (including 'A') are stored in a Hopfield network. The letter 'A' is not recovered.</center></i>\n",
    "\n",
    "1. Run the following code. Read the inline comments and look up the doc of functions you do not know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noisy_copy(template, noise_level):\n",
    "    \"\"\"\n",
    "    Creates a copy of the template pattern and reassigns N pixels. N is determined\n",
    "    by the noise_level\n",
    "    Note: reassigning a random value is not the same as flipping the state. This\n",
    "    function reassigns a random value.\n",
    "\n",
    "    Args:\n",
    "        template:\n",
    "        noise_level: a value in [0,1]. for 0, this returns a copy of the template.\n",
    "        for 1, a random pattern of the same size as template is returned.\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if noise_level == 0:\n",
    "        return template.copy()\n",
    "    if noise_level < 0 or noise_level > 1:\n",
    "        raise ValueError(\"noise level is not in [0,1] but {}0\".format(noise_level))\n",
    "    linear_template = template.copy().flatten()\n",
    "    n = np.prod(template.shape)\n",
    "    nr_mutations = int(round(n * noise_level))\n",
    "    idx_reassignment = np.random.choice(n, nr_mutations, replace=False)\n",
    "    rand_values = np.random.binomial(1, 0.5, nr_mutations)\n",
    "    rand_values = rand_values * 2 - 1  # map {0,1} to {-1, +1}\n",
    "    linear_template[idx_reassignment] = rand_values\n",
    "    return linear_template.reshape(template.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from neurodynex.hopfield_network import network, pattern_tools, plot_tools\n",
    "import numpy\n",
    "\n",
    "# the letters we want to store in the hopfield network\n",
    "letter_list = ['A', 'B', 'C', 'S', 'X', 'Y', 'Z']\n",
    "\n",
    "# set a seed to reproduce the same noise in the next run\n",
    "# numpy.random.seed(123)\n",
    "\n",
    "abc_dictionary = pattern_tools.load_alphabet()\n",
    "print(\"the alphabet is stored in an object of type: {}\".format(type(abc_dictionary)))\n",
    "# access the first element and get it's size (they are all of same size)\n",
    "pattern_shape = abc_dictionary['A'].shape\n",
    "print(\"letters are patterns of size: {}. Create a network of corresponding size\".format(pattern_shape))\n",
    "# create an instance of the class HopfieldNetwork\n",
    "hopfield_net = network.HopfieldNetwork(nr_neurons= pattern_shape[0]*pattern_shape[1])\n",
    "\n",
    "# create a list using Pythons List Comprehension syntax:\n",
    "pattern_list = [abc_dictionary[key] for key in letter_list]\n",
    "plot_tools.plot_pattern_list(pattern_list)\n",
    "\n",
    "# store the patterns\n",
    "hopfield_net.store_patterns(pattern_list)\n",
    "\n",
    "# create a noisy version of a pattern and use that to initialize the network\n",
    "noisy_init_state = get_noisy_copy(abc_dictionary['A'], noise_level=.2)\n",
    "hopfield_net.set_state_from_pattern(noisy_init_state)\n",
    "\n",
    "# from this initial state, let the network dynamics evolve.\n",
    "states = hopfield_net.run_with_monitoring(nr_steps=4)\n",
    "\n",
    "# each network state is a vector. reshape it to the same shape used to create the patterns.\n",
    "states_as_patterns = pattern_tools.reshape_patterns(states, pattern_list[0].shape)\n",
    "\n",
    "# plot the states of the network\n",
    "plot_tools.plot_state_sequence_and_overlap(\n",
    "    states_as_patterns, pattern_list, reference_idx=0, suptitle=\"Network dynamics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add the letter 'R' to the letter list and store it in the network. Is the pattern 'A' still a fixed point? Does the overlap between the network state and the reference pattern 'A' always decrease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Make a guess of how many letters the network can store. Then create a (small) set of letters. Check if **all** letters of your list are fixed points under the network dynamics. Explain the discrepancy between the network capacity C (computed above) and your observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Bonus\n",
    "\n",
    "The implementation of the Hopfield Network in `hopfield_network.network` offers a possibility to provide a custom update function `HopfieldNetwork.set_dynamics_to_user_function()`. \n",
    "\n",
    "1. Have a look at the source code of `HopfieldNetwork.set_dynamics_sign_sync()` to learn how the update dynamics are implemented. Then try to implement your own function. For example, you could implement an asynchronous update with stochastic neurons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
