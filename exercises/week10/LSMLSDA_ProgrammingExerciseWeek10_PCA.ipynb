{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction: tutorial\n",
    "Let's do some practical exercises with what we have learnt\n",
    "## Data generation \n",
    "We wish to generate synthetic 2D and 3D data to play with. \n",
    "\n",
    "Please read the carefully, make sure to understand what is happening here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:04:58.026660Z",
     "start_time": "2019-07-03T07:04:58.010221Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(743359) # random seed for consistency\n",
    "from numpy.random import multivariate_normal as MVN\n",
    "import matplotlib.pyplot as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.328700Z",
     "start_time": "2019-07-03T06:22:36.316712Z"
    }
   },
   "outputs": [],
   "source": [
    "# samples from a MVN\n",
    "def gen_gauss_prior(D=1, N=50):\n",
    "    zero_mean = np.zeros(D)\n",
    "    identity_cov = np.eye(D)\n",
    "    z = MVN(zero_mean, identity_cov, N)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.344365Z",
     "start_time": "2019-07-03T06:22:36.332723Z"
    }
   },
   "outputs": [],
   "source": [
    "# generates Gaussian observations data set \n",
    "def gen_gauss_obs(W, mu, zs, sigma):\n",
    "    \"\"\"Sample a dataset from p(x|z).\"\"\"\n",
    "    \n",
    "    N, D, M = len(zs), len(mu), zs.shape[1]\n",
    "    W, mu = np.asarray(W), np.asarray(mu)\n",
    "    print('{} {}-dim observations from {}-dim latents'.format(N, D, M))\n",
    "    #xs = np.empty((N, D))\n",
    "    #for m, z in enumerate(zs):\n",
    "    #    xs[m, :] = MVN(W@z + mu, sigma*np.eye(D))\n",
    "    xs = np.array([MVN(W@z + mu, sigma*np.eye(D)) for z in zs])\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.390805Z",
     "start_time": "2019-07-03T06:22:36.347716Z"
    }
   },
   "outputs": [],
   "source": [
    "zs = gen_gauss_prior(D=1, N=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.428997Z",
     "start_time": "2019-07-03T06:22:36.397631Z"
    }
   },
   "outputs": [],
   "source": [
    "xs = gen_gauss_obs([[1],[1]],mu=[3,1], zs=zs, sigma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.437781Z",
     "start_time": "2019-07-03T06:22:36.433275Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the mean along first dimensions \n",
    "xsmean = xs.mean(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.666001Z",
     "start_time": "2019-07-03T06:22:36.441686Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot data as scatter, plot mean\n",
    "ax = pl.subplot(111, label='by hand')\n",
    "ax.scatter(*xs.T) # short for unpack to xs[:,0], xs[:,1]\n",
    "ax.scatter(*xsmean.T, color='r', label='mean')\n",
    "ax.set_aspect('equal')\n",
    "plt.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA 'by hand'\n",
    "Note we actually rely on the primitives `np.cov` and `np.eigval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.671604Z",
     "start_time": "2019-07-03T06:22:36.667525Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: calculate covariance matrix of the data\n",
    "S = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.680208Z",
     "start_time": "2019-07-03T06:22:36.672964Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: compute eigenvectors and values\n",
    "ls, vs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.696210Z",
     "start_time": "2019-07-03T06:22:36.682003Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: check whether eigenvector are correct: what is the result of applying S to vs? What should it be? \n",
    "# if confused, check the definition of eigenvectors of a matrix. \n",
    "# note eigenvalues laid out in columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.706983Z",
     "start_time": "2019-07-03T06:22:36.700904Z"
    }
   },
   "outputs": [],
   "source": [
    "# optional: check correct, using similarity relation S - \\Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:36.723576Z",
     "start_time": "2019-07-03T06:22:36.709962Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: sort eigenvectors by eigenvalues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Transformed data\n",
    "Project data on eigenvector and plot it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:37.486854Z",
     "start_time": "2019-07-03T06:22:37.025037Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:37.501405Z",
     "start_time": "2019-07-03T06:22:37.490368Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "transform = pca.fit_transform(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:37.509398Z",
     "start_time": "2019-07-03T06:22:37.503815Z"
    }
   },
   "outputs": [],
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:37.517679Z",
     "start_time": "2019-07-03T06:22:37.512169Z"
    }
   },
   "outputs": [],
   "source": [
    "def ziptr(*args): \n",
    "    return list(zip(*args))\n",
    "# print each eigenvalue with its eigenvector\n",
    "es_by_sklearn= ziptr(pca.explained_variance_, pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:37.534225Z",
     "start_time": "2019-07-03T06:22:37.521485Z"
    }
   },
   "outputs": [],
   "source": [
    "es_by_hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:37.544620Z",
     "start_time": "2019-07-03T06:22:37.537952Z"
    }
   },
   "outputs": [],
   "source": [
    "# see StackOverflow question 44765682 for sign ambiguity\n",
    "es_by_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:31:37.098116Z",
     "start_time": "2019-07-03T06:31:36.847752Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.scatter(*zip(*(-transform)))\n",
    "pl.scatter(*zs)\n",
    "pl.axis('equal')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be surprised if the plot is the mirror image of the plot of the PCA by hand - there's ambiguity in the sign of the eigenvectors, since both signs indicate the same direction for the subspace. We can simply multiply the transformed data by (−1) to revert the mirror image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST PCA\n",
    "further reference: [colah notebook](https://colah.github.io/posts/2014-10-Visualizing-MNIST) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sphering with PCA\n",
    "Typical pre-processing step: normalize data to have zero mean and unit variance.\n",
    "\n",
    "With PCA one can add decorrelation of features - i.e. rewrite features in t®ms of orthogonal basis vectors.\n",
    "\n",
    "This is a dataset rotation, i.e. no dimensionality reduction, $M=D$, into a basis in which its covariance matrix is diagonal.\n",
    "\n",
    "Task: \n",
    "\n",
    "1. generate a dataset. \n",
    "2. center and rotate (decorrelate features) by applying $$y_n = U^\\top (x_n - \\bar{x})$$\n",
    "3. Rescale projected values to have unit variance: $$\\tilde{y}_n = \\Lambda^{-1/2} U^\\top (x_n-\\bar{x})$$\n",
    "4. How to do this when using sklearn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for compression: faces\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "\n",
    "\n",
    "Load the olivetti faces dataset from sklearn: 400 images each of which has $64^2$ pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:37.968847Z",
     "start_time": "2019-07-03T06:22:37.778273Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "faces = datasets.fetch_olivetti_faces()\n",
    "faces.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data we're working with - note faces have been localized and scaled to a common size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:27:01.523543Z",
     "start_time": "2019-07-03T06:27:00.982142Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "# plot several images\n",
    "for i in range(15):\n",
    "    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(faces.images[i], cmap=plt.cm.bone)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:38.277079Z",
     "start_time": "2019-07-03T06:22:38.273453Z"
    },
    "collapsed": true
   },
   "source": [
    "Reduce the thousands of features to just 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:27:03.877762Z",
     "start_time": "2019-07-03T06:27:03.722216Z"
    }
   },
   "outputs": [],
   "source": [
    "efaces = PCA(n_components=150, whiten=True)\n",
    "efaces.fit(faces.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:27:05.585603Z",
     "start_time": "2019-07-03T06:27:04.447610Z"
    }
   },
   "outputs": [],
   "source": [
    "print(efaces.components_.shape)\n",
    "fig = pl.figure(figsize=(16, 6))\n",
    "for i in range(30):\n",
    "    ax = fig.add_subplot(3, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(efaces.components_[i].reshape(faces.images[0].shape),\n",
    "              cmap=plt.cm.bone)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the components that explain the highest variance first tackle global illumination,and then progressively finer and finer features (eyebrows, eyes, lips...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For subsequent use (i.e. classification) we can use not the original images ($64^2$ dimensions) but just the coefficients to these eigenfaces (150). These are given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:32:38.456135Z",
     "start_time": "2019-07-03T06:32:38.388308Z"
    }
   },
   "outputs": [],
   "source": [
    "facescores = efaces.transform(faces.data)\n",
    "facescores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase: probabilistic models can be cross-validated for the number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:22:38.279605Z",
     "start_time": "2019-07-03T06:22:35.937Z"
    }
   },
   "outputs": [],
   "source": [
    "# From the sklearn documentation\n",
    "# Authors: Alexandre Gramfort\n",
    "#          Denis A. Engemann\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.covariance import ShrunkCovariance, LedoitWolf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# #############################################################################\n",
    "# Create the data\n",
    "\n",
    "n_samples, n_features, rank = 1000, 50, 10\n",
    "sigma = 1.\n",
    "rng = np.random.RandomState(42)\n",
    "U, _, _ = linalg.svd(rng.randn(n_features, n_features))\n",
    "X = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)\n",
    "\n",
    "# Adding homoscedastic noise\n",
    "X_homo = X + sigma * rng.randn(n_samples, n_features)\n",
    "\n",
    "# Adding heteroscedastic noise\n",
    "sigmas = sigma * rng.rand(n_features) + sigma / 2.\n",
    "X_hetero = X + rng.randn(n_samples, n_features) * sigmas\n",
    "\n",
    "# #############################################################################\n",
    "# Fit the models\n",
    "\n",
    "n_components = np.arange(0, n_features, 5)  # options for n_components\n",
    "\n",
    "\n",
    "def compute_scores(X):\n",
    "    pca = PCA(svd_solver='full')\n",
    "    fa = FactorAnalysis()\n",
    "\n",
    "    pca_scores, fa_scores = [], []\n",
    "    for n in n_components:\n",
    "        pca.n_components = n\n",
    "        fa.n_components = n\n",
    "        pca_scores.append(np.mean(cross_val_score(pca, X, cv=5)))\n",
    "        fa_scores.append(np.mean(cross_val_score(fa, X, cv=5)))\n",
    "\n",
    "    return pca_scores, fa_scores\n",
    "\n",
    "\n",
    "def shrunk_cov_score(X):\n",
    "    shrinkages = np.logspace(-2, 0, 30)\n",
    "    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages}, cv=5)\n",
    "    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X, cv=5))\n",
    "\n",
    "\n",
    "def lw_score(X):\n",
    "    return np.mean(cross_val_score(LedoitWolf(), X, cv=5))\n",
    "\n",
    "\n",
    "for X, title in [(X_homo, 'Homoscedastic Noise'),\n",
    "                 (X_hetero, 'Heteroscedastic Noise')]:\n",
    "    pca_scores, fa_scores = compute_scores(X)\n",
    "    n_components_pca = n_components[np.argmax(pca_scores)]\n",
    "    n_components_fa = n_components[np.argmax(fa_scores)]\n",
    "\n",
    "    pca = PCA(svd_solver='full', n_components='mle')\n",
    "    pca.fit(X)\n",
    "    n_components_pca_mle = pca.n_components_\n",
    "\n",
    "    print(\"best n_components by PCA CV = %d\" % n_components_pca)\n",
    "    print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\n",
    "    print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(n_components, pca_scores, 'b', label='PCA scores')\n",
    "    plt.plot(n_components, fa_scores, 'r', label='FA scores')\n",
    "    plt.axvline(rank, color='g', label='TRUTH: %d' % rank, linestyle='-')\n",
    "    plt.axvline(n_components_pca, color='b',\n",
    "                label='PCA CV: %d' % n_components_pca, linestyle='--')\n",
    "    plt.axvline(n_components_fa, color='r',\n",
    "                label='FactorAnalysis CV: %d' % n_components_fa,\n",
    "                linestyle='--')\n",
    "    plt.axvline(n_components_pca_mle, color='k',\n",
    "                label='PCA MLE: %d' % n_components_pca_mle, linestyle='--')\n",
    "\n",
    "    # compare with other covariance estimators\n",
    "    plt.axhline(shrunk_cov_score(X), color='violet',\n",
    "                label='Shrunk Covariance MLE', linestyle='-.')\n",
    "    plt.axhline(lw_score(X), color='orange',\n",
    "                label='LedoitWolf MLE' % n_components_pca_mle, linestyle='-.')\n",
    "\n",
    "    plt.xlabel('nb of components')\n",
    "    plt.ylabel('CV scores')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:40:10.161282Z",
     "start_time": "2019-07-03T06:40:10.137219Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "# print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:40:12.830507Z",
     "start_time": "2019-07-03T06:40:12.822247Z"
    },
    "collapsed": true
   },
   "source": [
    "D = 30  (smoothness, radius etc.) N=569, 212 are malignant (encoded 0) and 357 benign (encoded 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot axis aligned histograms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:44:22.591790Z",
     "start_time": "2019-07-03T06:44:22.559526Z"
    }
   },
   "outputs": [],
   "source": [
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:48:24.655570Z",
     "start_time": "2019-07-03T06:48:17.064619Z"
    }
   },
   "outputs": [],
   "source": [
    "N, D = cancer.data.shape\n",
    "fig, axes = pl.subplots(10, 3, figsize=(12, 9)) \n",
    "malignant = cancer.data[cancer.target==0] \n",
    "benign = cancer.data[cancer.target==1]\n",
    "ax = axes.ravel()# flatten axes for iteration\n",
    "for i, name in enumerate(cancer.feature_names):\n",
    "  _, bins = np.histogram(cancer.data[:,i], bins=40)\n",
    "  ax[i].hist(malignant[:,i], bins=bins, color='r', alpha=.5)\n",
    "  ax[i].hist(benign[:,i], bins=bins, color='g', alpha=0.3)\n",
    "  ax[i].set_title(name,fontsize=9)\n",
    "  ax[i].axes.get_xaxis().set_visible(False)\n",
    "  ax[i].set_yticks(())\n",
    "ax[0].legend(['malignant','benign'], loc='best', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features have a great classification potential, others not so much. Worse, some are semi-redundant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:50:28.828275Z",
     "start_time": "2019-07-03T06:50:27.742716Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cancer_df = pd.DataFrame(cancer.data, columns = cancer.feature_names)\n",
    "pl.subplot(1,2,1)\n",
    "pl.scatter(cancer_df['worst symmetry'], cancer_df['worst texture'], s=cancer_df['worst area']*0.05, color='magenta', label='check', alpha=0.3)\n",
    "pl.xlabel('Worst Symmetry',fontsize=12)\n",
    "pl.ylabel('Worst Texture',fontsize=12)\n",
    "pl.subplot(1,2,2)\n",
    "pl.scatter(cancer_df['mean radius'], cancer_df['mean concave points'], s=cancer_df['mean area']*0.05, color='purple', label='check', alpha=0.3)\n",
    "pl.xlabel('Mean Radius',fontsize=12)\n",
    "pl.ylabel('Mean Concave Points',fontsize=12)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:54:23.395549Z",
     "start_time": "2019-07-03T06:54:22.843881Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = pl.subplots(1, 2, figsize=(12, 9)) \n",
    "cancer_df.plot.scatter(x='worst symmetry', y='worst texture', ax=axes[0])\n",
    "cancer_df.plot.scatter(x='mean radius', y='mean concave points', ax=axes[1])\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:02:21.000079Z",
     "start_time": "2019-07-03T07:02:19.592536Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# industrialize\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(cancer_df.iloc[:, 0:30:7], \n",
    "               c=cancer.target, marker='.', alpha=0.8)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:07:33.179994Z",
     "start_time": "2019-07-03T07:07:33.159209Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(cancer.data)\n",
    "cancer_pca = PCA(n_components=3) \n",
    "z_cancer = cancer_pca.fit_transform(X_scaled)\n",
    "z_cancer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:13:58.321203Z",
     "start_time": "2019-07-03T07:13:58.103976Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = dict(enumerate(['red', 'green']))\n",
    "labels = dict(enumerate(['Malignant', 'Benign']))\n",
    "marker = dict(enumerate(['*', 'o']))\n",
    "alpha = dict(enumerate([.3, .5]))\n",
    "fig,ax = pl.subplots(figsize=(7,5))\n",
    "fig.patch.set_facecolor('white')\n",
    "zx, zy = z_cancer[:,0], z_cancer[:,1]\n",
    "for t in np.unique(cancer.target):\n",
    "    ix = np.where(cancer.target==t)\n",
    "    ax.scatter(zx[ix],zy[ix],c=colors[t],s=40,\n",
    "               label=labels[t],marker=marker[t],alpha=alpha[t])\n",
    "pl.xlabel(\"First Principal Component\",fontsize=14)\n",
    "pl.ylabel(\"Second Principal Component\",fontsize=14)\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:15:42.702763Z",
     "start_time": "2019-07-03T07:15:42.346552Z"
    }
   },
   "outputs": [],
   "source": [
    "# heat plot for mixing\n",
    "pl.matshow(cancer_pca.components_,cmap='viridis')\n",
    "pl.yticks([0,1,2],['1st Comp','2nd Comp','3rd Comp'],\n",
    "          fontsize=10)\n",
    "pl.colorbar()\n",
    "pl.xticks(range(len(cancer.feature_names)),\n",
    "          cancer.feature_names, rotation=65, ha='left')\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T07:16:42.446013Z",
     "start_time": "2019-07-03T07:16:42.061606Z"
    }
   },
   "outputs": [],
   "source": [
    "# worst features\n",
    "feature_worst=list(cancer_df.columns[20:31]) \n",
    "import seaborn as sns\n",
    "s=sns.heatmap(cancer_df[feature_worst].corr(),cmap='coolwarm') \n",
    "s.set_yticklabels(s.get_yticklabels(),rotation=30,fontsize=7)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=30,fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for vis, denoising: handwritten digits\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "\n",
    "# PCA for understandings; cancer dataset\n",
    "https://towardsdatascience.com/dive-into-pca-principal-component-analysis-with-python-43ded13ead21\n",
    "# PCA for preprocessing - importance downstream of feature scaling\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T06:25:35.593041Z",
     "start_time": "2019-07-03T06:25:35.573233Z"
    }
   },
   "source": [
    "# References\n",
    "* https://towardsdatascience.com/dive-into-pca-principal-component-analysis-with-python-43ded13ead21\n",
    "* https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_eigenfaces.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
