{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation-based inference on Hodgkin-Huxley model\n",
    "\n",
    "In this tutorial, we will infer the posterior for a Hodgkin-Huxley (HH) model through simulation-based inference. The number of spikes that a HH model produces crucially depends on the peak conductances of $g_{Na}$ and $g_{K}$.\n",
    "\n",
    "We will infer the space of parameters $g_{Na}$ and $g_{K}$ leading to 0, 1, 2, 3, 4, or more spikes for a noisy step current input. The Hodgkin-Huxley model will be implemented in Brian, we'll train a neural density estimator in PyTorch, and finally infer the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements \n",
    "\n",
    "Before getting started, you will need to install brian and PyTorch. Installation depends on your operation system and package manager. See instructions given at [brian docs](https://brian2.readthedocs.io/en/stable/introduction/install.html), and [PyTorch get started](https://pytorch.org/get-started/locally/). In addition, we will use a package called [tqdm](https://github.com/tqdm/tqdm) to display progress-bars during training.\n",
    "\n",
    "[Google Colab](https://colab.research.google.com/) and [Binder](https://mybinder.org) offer free hosted notebook solutions. If you are running this tutorial on either of those services, install packages by making a new cell in which you write and execute: `!pip install brian2 torch tqdm --user`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing installation\n",
    "\n",
    "If the installation was successful, imports in the next code cell should work out fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brian2\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "print('imported brian version {}'.format(brian2.__version__))\n",
    "print('imported torch version {}'.format(torch.__version__))\n",
    "print('imported tqdm version {}'.format(tqdm.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating HH with two free parameters in Brian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, we create a Hodgkin-Huxley simulator in brian. The Hodgkin-Huxley model is specified through a set of differential equations. For background on the model, refer to [the original publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/), or textbooks like [Izhikevich's](https://www.izhikevich.org/publications/dsn/index.htm) (chapter 2.3).\n",
    "\n",
    "In the `simulate()` function below we use Brian to simulate a HH neuron model. We set the relevant fixed parameters, define the HH equations as a string, as common in Brian, and then we simulate the model for a given set of parameters. \n",
    "\n",
    "In our setup we have two free parameters: the peak conductances for the sodium and the potassium currents, $g_{Na}$ and $g_K$. The purpose of the function below is to simulate the HH model, given a set of parameters $(g_{Na}, g_K)$.  \n",
    "\n",
    "To simalate efficiently, we apply a trick: Instead of running one Brian simulation for every pair of parameters we define a brian Network with many neurons, all following the HH dynamics, and without any connectivity / synapses between them. Given a set of many parameter tuples $\\{(g_{Na}, g_K)\\}_{i=1}^N$ we define a network with $N$ neurons, in which every neuren has different values for $(g_{Na}, g_K)$. This way we exploit the fact that Brian is optimized for network simulations and we simulate $N$ parameter tuples in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "\n",
    "\n",
    "def simulate(params):\n",
    "    \"\"\"Simulate HH model for given set of parameters (params)\n",
    "\n",
    "    Runs params.shape[0] brian HH simulations in parallel to generate voltage traces \n",
    "    and record spike times. \n",
    "\n",
    "    Parameters: \n",
    "        params: np.array with shape=(n_params, dim_params)\n",
    "        record_traces: Boolean, flag for recording voltage traces (True) or just spike times. \n",
    "\n",
    "    Returns: \n",
    "        statemonitor, spikemonitor: brian statemonitor and spikemonitor if record_traces=True. \n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    area = 20000*umetre**2\n",
    "    Cm = 1*ufarad*cm**-2 * area  # membrane capacitance \n",
    "    gl = 5e-5*siemens*cm**-2 * area  # leak conductance \n",
    "    # leak, Na, K, reversal potentials\n",
    "    El = -65*mV  \n",
    "    EK = -90*mV\n",
    "    ENa = 50*mV\n",
    "    VT = -63*mV  # threshold value for gating functions\n",
    "    Vrefr = -40*mV  # refractory voltage\n",
    "    Vthres = -40*mV  # spiking threshold\n",
    "\n",
    "    # Model equations\n",
    "    eqs = Equations('''\n",
    "        # Voltage\n",
    "        dv/dt = (gl*(El-v) - g_na*(m*m*m)*h*(v-ENa) - g_kd*(n*n*n*n)*(v-EK) + I)/Cm: volt\n",
    "        \n",
    "        # Gating variables\n",
    "        dm/dt = 0.32*(mV**-1)*(13.*mV-v+VT)/\n",
    "          (exp((13.*mV-v+VT)/(4.*mV))-1.)/ms*(1-m)-0.28*(mV**-1)*(v-VT-40.*mV)/\n",
    "          (exp((v-VT-40.*mV)/(5.*mV))-1.)/ms*m : 1\n",
    "        dn/dt = 0.032*(mV**-1)*(15.*mV-v+VT)/\n",
    "          (exp((15.*mV-v+VT)/(5.*mV))-1.)/ms*(1.-n)-.5*exp((10.*mV-v+VT)/(40.*mV))/ms*n : 1\n",
    "        dh/dt = 0.128*exp((17.*mV-v+VT)/(18.*mV))/ms*(1.-h)-4./(1+exp((40.*mV-v+VT)/(5.*mV)))/ms*h : 1\n",
    "\n",
    "        # Constant free parameters: Na conductance, K conductance and input current I\n",
    "        g_na : siemens  \n",
    "        g_kd : siemens\n",
    "        I : amp\n",
    "        ''')\n",
    "    \n",
    "    # Stimulus\n",
    "    duration = 120 * ms  # simulation time \n",
    "    t_on = 10 * ms  # stimulus onset \n",
    "    t_off = duration - t_on  # offset \n",
    "\n",
    "    # Stimulus strength\n",
    "    current_strength = 1e-4*uA  #mu Ampere\n",
    "\n",
    "    # Noise strength\n",
    "    sigma_I = 1 * current_strength\n",
    "\n",
    "    # Trick: Network, but no connectivity\n",
    "    n_neurons = params.shape[0]\n",
    "    \n",
    "    group = NeuronGroup(n_neurons, eqs,\n",
    "                        threshold='v > Vthres',\n",
    "                        refractory='v > Vrefr', \n",
    "                        method='exponential_euler')\n",
    "    \n",
    "    # Another trick to have noise on the input current: \n",
    "    # Change the input current every 0.1 ms: if in valid t range, use current strength plus scaled N(0, 1) noise\n",
    "    group.run_regularly('I = (t_on<t) * (t<t_off) * current_strength + sigma_I * randn()', dt=0.1*ms)\n",
    "\n",
    "    # Get the batch of (gna, gk) parameter tuples from the input \n",
    "    g_na_values = params[:, 0]\n",
    "    g_kd_values = params[:, 1]\n",
    "\n",
    "    # Set initial voltage\n",
    "    group.v = El\n",
    "    \n",
    "    # Set different (gna, gk) values\n",
    "    group.g_na = g_na_values * msiemens*cm**-2 * area\n",
    "    group.g_kd = g_kd_values * msiemens*cm**-2 * area\n",
    "\n",
    "    # Statemonitor allows to monitor traces\n",
    "    statemonitor = StateMonitor(group, ['v', 'I'], record=True)\n",
    "\n",
    "    net = Network([group, statemonitor])\n",
    "    net.run(duration)\n",
    "\n",
    "    return statemonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... trying it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "params = np.array([[0.1, 0.1]])\n",
    "statemon = simulate(params)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, figsize=(15, 8))\n",
    "plt.sca(ax[0])\n",
    "plt.plot(statemon.t/ms, statemon.v.T/mV)\n",
    "plt.ylabel('voltage [mV]')\n",
    "plt.sca(ax[1])\n",
    "plt.plot(statemon.t/ms, statemon.I.T/mA)\n",
    "plt.ylabel('current [mA]')\n",
    "plt.xlabel('time [ms]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. The above model works, but doesn't produce spikes. Can you identify a regime where it produces at least one spike by hand by playing around with the parameters manually? \n",
    "2. The model returns voltage and current traces. However, ultimately, we are interested in the number of spikes the model produces. Can you change the model such that it returns the spike count instead? \n",
    "Hint: You can use brian to do this, rather than writing your own function for detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue, copy the `simulate()` function from the following Gist and paste it in the cell below:\n",
    "\n",
    "https://gist.github.com/jan-matthis/883c4a4c5b670a8ba575674b2188216d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a uniform prior over our parameters:\n",
    "$$g_{Na} \\sim Uniform[5, 120] \\\\ \n",
    "g_{K} \\sim Uniform[1, 50]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_gNa = [5, 120.]\n",
    "lim_gK = [1, 50.]\n",
    "\n",
    "def prior(num_samples=10):\n",
    "    gNa = np.random.uniform(lim_gNa[0], lim_gNa[1], size=(num_samples,))\n",
    "    gK = np.random.uniform(lim_gK[0], lim_gK[1], size=(num_samples,))\n",
    "    inputs = np.stack([gNa, gK], axis=1)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `prior(num_samples)` will return an matrix of dimensions `(num_samples, 2)`, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior(num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use those parameters with the HH model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = simulate(prior(num_samples=10))\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a vector $s$ containing spike counts. Each of its elements $n$ is drawn from $p(s_n|g_{{Na}_n}, g_{{K}_n})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to infer the (approximate) posteriors over spike counts, e.g., $p(g_{Na}, g_{K}|s=3)$, to answer the questions which combinations of parameters lead to exactly three spikes. By Bayes theorem, the posterior is proportional to likelihood times prior, e.g., $\\mathcal{L}(\\theta) = p(s=3|g_{Na}, g_{K}))$, times a uniform density. The problem is that we cannot evaluate the likelihood for the model we specified in brian above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation-based inference by density estimation\n",
    "\n",
    "What we can do, however, is generate samples from the model: We will generate a dataset of consisting of parameters and spike counts. We will use this dataset to train a density estimator $q(s|g_{Na}, g_{K})$. In difference to $p(s|g_{Na}, g_{K})$, we can evaluate the pdf of $q$. This will help us to infer the posterior: $q$ acts as an approximation/surrogate to the true likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different kinds of density estimators. A flexible class of density estimators uses artificial neural networks (NNs). We will set up a NN in PyTorch in order to regress from input parameters $g_{Na}, g_{K}$ onto predicted number of spikes. More specifically, the regression will be onto a vector of probabilities parametrizing a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 1000\n",
    "parameters = prior(num_samples=num_simulations)\n",
    "spike_counts = simulate(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a NN in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will set up a NN in PyTorch. To follow along, it will not be essential to know the details of how PyTorch works. If you want to read more about it, refer to the [PyTorch tutorials](https://pytorch.org/tutorials/). You can also easily find countless blog posts and video lectures online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "num_spikes_categories = 6\n",
    "num_hiddens = [200, 200]\n",
    "\n",
    "# The NN takes 2 parameters as input\n",
    "# There are 2 hidden layers of 200 units each\n",
    "# The regression is onto 6 numbers that will represent the probabilities of\n",
    "# 0, 1, 2, ... 5 (or more) spikes:\n",
    "network = nn.Sequential(\n",
    "    nn.Linear(2, num_hiddens[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(num_hiddens[0], num_hiddens[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(num_hiddens[1], num_spikes_categories),\n",
    "    )\n",
    "\n",
    "# The last category represents 5 or spikes\n",
    "# Therefore, we threshold spikes at 5 max:\n",
    "spike_counts[spike_counts>5] = 5\n",
    "\n",
    "# Convert the training data into a torch.tensors, and create a TensorDataset which we will use for training\n",
    "# From a TensorDataset, we construct a DataLoader that allows splitting data into shuffled batches\n",
    "# during training of the NN. For background see PyTorch tutorials on data handling\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(torch.tensor(parameters, dtype=torch.float32), \n",
    "                           torch.tensor(spike_counts, dtype=torch.float32))\n",
    "data_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "\n",
    "\n",
    "# Create an optimizer\n",
    "optim = torch.optim.Adam(network.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "# import tqdm for progress bars\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "# Train the parameters of the NN in minibatches\n",
    "losses = []\n",
    "for epoch in tqdm_notebook(range(50)):\n",
    "    for inputs, outputs in data_loader:\n",
    "        logits = network(inputs)\n",
    "        cond_dist = torch.distributions.Categorical(logits=logits)\n",
    "        loss = -1. * cond_dist.log_prob(outputs.reshape(-1)).mean()\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "# Plot loss of NN training\n",
    "plt.title('loss')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the NN, we can infer the posteriors by evaluation on a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_res = 50  # resolution of grid\n",
    "grid_linspaces = [np.linspace(*lim_gNa, grid_res), np.linspace(*lim_gK, grid_res)]\n",
    "grids = np.stack(np.meshgrid(*grid_linspaces), axis=2)  # (grid_res, grid_res, 2)\n",
    "grids_flat = grids.reshape(-1, 2)  # (grid_res**2, 2)\n",
    "\n",
    "logits = network.forward(torch.tensor(grids_flat, dtype=torch.float32))\n",
    "cond_dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "probs_np = cond_dist.probs.detach().numpy().reshape(\n",
    "    grid_res, grid_res, num_spikes_categories)\n",
    "\n",
    "for num_spikes in range(num_spikes_categories):\n",
    "    # probs_np contains likelihoods\n",
    "    # to infer posterior probabilities, we need to multiply likelihood times prior (uniform in our example)\n",
    "    # and calculate the normalizing factor Z of the posterior (the posterior pdf needs to integrate to 1):\n",
    "    Z = probs_np[:, :, num_spikes] \n",
    "    Z *= grid_res/(lim_gNa[1] - lim_gNa[0]) \n",
    "    Z *= grid_res/(lim_gK[1] - lim_gK[0])\n",
    "    Z = np.sum(Z)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(r'$p(g_{Na}, g_{K}|s=' + str(num_spikes) + r')$')\n",
    "    \n",
    "    plt.imshow(probs_np[:,:,num_spikes]/Z, origin='lower')\n",
    "    \n",
    "    plt.xticks(np.linspace(0, 50, 5), np.linspace(lim_gNa[0], lim_gNa[1], 5))\n",
    "    plt.xlabel('gNa')\n",
    "    \n",
    "    plt.yticks(np.linspace(0, 50, 5), np.linspace(lim_gK[0], lim_gK[1], 5))\n",
    "    plt.ylabel('gK')\n",
    "    \n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercises:\n",
    "* Pick two other parameters for which you infer the posterior: Modify simulator and priors accordingly\n",
    "* Find a way to draw samples from the posterior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
